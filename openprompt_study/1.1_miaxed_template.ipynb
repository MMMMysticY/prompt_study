{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# mixed template的使用\n",
    "mixed template是写template的一种方法\n",
    "{\"soft\"}方法是除了mask的一种token 在训练时也需要更新 可以指定初始化值\n",
    "同时{\"soft\"}的参数和模型的参数单独更新"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "import os\n",
    "import jsonlines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/wy/datasets/huggingface_datasets/super_glue'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/wy/datasets/huggingface_datasets/'\n",
    "cb_data_path = os.path.join(data_path, 'super_glue')\n",
    "cb_data_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (/home/wy/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58ba1ac522b340a8af016032048fe16c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(cb_data_path,'cb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 250\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 56\n    })\n    test: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 250\n    })\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = list()\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a=data['premise'], text_b=data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"guid\": 0,\n  \"label\": 0,\n  \"meta\": {},\n  \"text_a\": \"It was a complex language. Not written down but handed down. One might say it was peeled down.\",\n  \"text_b\": \"the language was peeled down\",\n  \"tgt_text\": null\n}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from openprompt.plms import load_plm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MixedTemplate\n",
    "使用自定义的template而非手动定义的template\n",
    "使用{\"soft\"}关键词 如果指定了value值 就使用value值初始化 否则会随机初始化token\n",
    "mask部分必然在训练过程中要被更新推理；而soft token也要被更新 而且是单独地更新"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from openprompt.prompts import MixedTemplate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "my_template1 = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\": \"Question\"} {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.')\n",
    "my_template = MixedTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"placeholder\":\"text_b\"} {\"soft\"} {\"mask\"}.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[[{'text': 'It was a complex language. Not written down but handed down. One might say it was peeled down.',\n   'soft_token_ids': 0,\n   'loss_ids': 0,\n   'shortenable_ids': 1},\n  {'text': '', 'soft_token_ids': 1, 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': '', 'soft_token_ids': 2, 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': '', 'soft_token_ids': 3, 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': ' the language was peeled down',\n   'soft_token_ids': 0,\n   'loss_ids': 0,\n   'shortenable_ids': 1},\n  {'text': '', 'soft_token_ids': 4, 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': '<mask>', 'soft_token_ids': 0, 'loss_ids': 1, 'shortenable_ids': 0},\n  {'text': '.', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 0}],\n {'guid': 0, 'label': 0}]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_example = my_template.wrap_one_example(dataset['train'][0])\n",
    "wrapped_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "wrapped_t5tokenizer= WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer, truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from openprompt import  PromptDataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 250it [00:00, 814.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = PromptDataLoader(dataset=dataset['train'], template=my_template, tokenizer=tokenizer,\n",
    "                                tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "                                batch_size=4, shuffle=True, teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "my_verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=3, label_words=[[\"yes\"], [\"no\"], [\"maybe\"]])\n",
    "# 数据集中的idx 0->蕴含(True) 1->矛盾(False) 2->中立(Maybe) 所以要与数据集的idx对应 就用\"yes\", \"no\", \"maybe\"进行处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm, template=my_template, verbalizer=my_verbalizer, freeze_plm=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    prompt_model.cuda(device=\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# bias和layernorm weight的weight decay为0是一个不错的trick"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 更新template的参数\n",
    "由于template中使用了{\"soft\"} 需要更新参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters2 = [\n",
    "    {'params': [p for n,p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 1.2346927225589752\n",
      "Epoch 1, average loss: 0.061103944666683674\n",
      "Epoch 2, average loss: 0.0021116542629897594\n",
      "Epoch 3, average loss: 0.009901425335556269\n",
      "Epoch 4, average loss: 0.002038600061496254\n",
      "Epoch 5, average loss: 0.00015941643505357206\n",
      "Epoch 6, average loss: 0.000683567437590682\n",
      "Epoch 7, average loss: 0.000541612840606831\n",
      "Epoch 8, average loss: 4.772607462655287e-05\n",
      "Epoch 9, average loss: 0.00011519958388817031\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for step, inputs in enumerate(train_loader):\n",
    "        # print(inputs)\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        total_loss = total_loss + loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, total_loss/(step+1)), flush=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 56it [00:00, 507.43it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataloader = PromptDataLoader(dataset=dataset['validation'], template=my_template, tokenizer=tokenizer,\n",
    "                                         tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "                                         batch_size=4, shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "                                         truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "all_preds = list()\n",
    "all_labels = list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    lables = inputs['label']\n",
    "    all_labels.extend(lables.cpu().tolist())\n",
    "    all_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8928571428571429"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = sum([int(i==j) for i,j in zip(all_preds, all_labels)])/len(all_preds)\n",
    "acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}