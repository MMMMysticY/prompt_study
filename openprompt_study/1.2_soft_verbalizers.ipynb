{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SoftVerbalizer的使用\n",
    "SoftVerbalizer api不同于ManualVerbalizer方法 其是可以更新的映射关系的一种\n",
    "可以使用label_words进行初始化 也可以不初始化\n",
    "最终注意要更新Verbalizer部分的参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/wy/OpenPrompt/datasets/TextClassification/agnews'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/wy/OpenPrompt/datasets/TextClassification/agnews'\n",
    "data_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "dataset['train'] = AgnewsProcessor().get_train_examples(data_dir=data_path)\n",
    "# ag_news数据集输入一段文本(text_a:题目 text_b:内容) 输出为新闻的分类 0->world 1->sports 2->business 3->sci/tech"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "120000"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"guid\": \"0\",\n  \"label\": 2,\n  \"meta\": {},\n  \"text_a\": \"Wall St. Bears Claw Back Into the Black (Reuters)\",\n  \"text_b\": \"Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\",\n  \"tgt_text\": null\n}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from openprompt.data_utils.data_sampler import FewShotSampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "sampler = FewShotSampler(num_examples_per_label=16, num_examples_per_label_dev=16, also_sample_dev=True)\n",
    "dataset['train'], dataset['validation'] = sampler(dataset['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "64"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "64"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['validation'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "dataset['test'] = AgnewsProcessor().get_test_examples(data_dir=data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "7600"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from openprompt.plms import load_plm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "my_template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} this is a kind of {\"mask\"} news.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "[[{'text': 'Japanese Leader Reshuffles Cabinet (AP)',\n   'loss_ids': 0,\n   'shortenable_ids': 1},\n  {'text': ' AP - Prime Minister Junichiro Koizumi replaced key ministers and ruling party leaders in a shuffle Monday aimed at solidifying his power and building momentum for his troubled reforms program.',\n   'loss_ids': 0,\n   'shortenable_ids': 1},\n  {'text': ' this is a kind of', 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0},\n  {'text': ' news.', 'loss_ids': 0, 'shortenable_ids': 0}],\n {'guid': '40948', 'label': 0}]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_example = my_template.wrap_one_example(dataset['train'][0])\n",
    "wrapped_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from openprompt import  PromptDataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 818.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = PromptDataLoader(dataset=dataset['train'], template=my_template, tokenizer=tokenizer,\n",
    "                                tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "                                batch_size=4, shuffle=True, teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自定义verbalizer\n",
    "在新闻文本分类任务中，输入的目标需要自定义 即 0->politics 1->sports 2->business 3->sci/tech\n",
    "可以提供label_words也可以不提供\n",
    "注意verbalizer部分的参数也需要更新"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "from openprompt.prompts import SoftVerbalizer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label word for a class is a list, only use the first word.\n",
      "Label word for a class is a list, only use the first word.\n",
      "Label word for a class is a list, only use the first word.\n",
      "Label word for a class is a list, only use the first word.\n"
     ]
    }
   ],
   "source": [
    "my_verbalizer = SoftVerbalizer(tokenizer=tokenizer, model=plm, num_classes=4, label_words=[[\"politics\", \"world\"], ['sports'], [\"business\"], [\"technology\", \"scientific\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm, template=my_template, verbalizer=my_verbalizer, freeze_plm=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    prompt_model.cuda(device=\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# bias和layernorm weight的weight decay为0是一个不错的trick"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters2 = [\n",
    "    {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
    "    {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.560626983642578\n",
      "27.742273330688477\n",
      "22.962292671203613\n",
      "21.80924916267395\n",
      "17.44739933013916\n",
      "15.008693099021912\n",
      "13.449524777276176\n",
      "14.033050328493118\n",
      "12.563024255964491\n",
      "12.50866894721985\n",
      "11.63405028256503\n",
      "10.664546092351278\n",
      "9.844750748696523\n",
      "9.144311500879537\n",
      "8.534802080357137\n",
      "8.015780330788402\n",
      "5.521378993988037\n",
      "3.0807735323905945\n",
      "2.6750882466634116\n",
      "2.0063161924481387\n",
      "1.6588308870792385\n",
      "2.0093927433093386\n",
      "1.7230449751950798\n",
      "1.5076643532956948\n",
      "1.388514939850817\n",
      "1.2496634458657352\n",
      "1.1368506084704262\n",
      "1.7088437795561429\n",
      "2.0403250840922387\n",
      "1.8966455596299576\n",
      "1.7702025223212938\n",
      "2.0603637814929243\n",
      "0.0\n",
      "0.0\n",
      "5.015214749922355e-05\n",
      "0.00013770697842119262\n",
      "0.00011889690504176542\n",
      "0.15037505621997602\n",
      "0.6275872230128569\n",
      "0.5491388201362497\n",
      "0.6485014676735672\n",
      "0.5842461179381644\n",
      "0.7499025798824732\n",
      "0.6875236996390109\n",
      "0.9157829439069386\n",
      "0.8503698764850144\n",
      "0.7936785513860135\n",
      "0.7440736419243876\n",
      "0.0\n",
      "2.9802318834981634e-08\n",
      "1.9868212556654424e-08\n",
      "2.5331928377170243e-07\n",
      "2.0861589113962964e-07\n",
      "1.7384657594969136e-07\n",
      "1.5752629905258736e-07\n",
      "7.184185210262761e-05\n",
      "6.450513354769625e-05\n",
      "6.557616029407853e-05\n",
      "6.023240364831524e-05\n",
      "5.5213036677622305e-05\n",
      "5.0965880010112894e-05\n",
      "6.779217996299991e-05\n",
      "6.330051677695773e-05\n",
      "5.934423447839787e-05\n",
      "0.0\n",
      "7.569560693809763e-06\n",
      "5.284792033914225e-06\n",
      "3.963594025435668e-06\n",
      "3.1708752203485345e-06\n",
      "2.751671056936781e-06\n",
      "2.4699070430805087e-05\n",
      "2.161168662695445e-05\n",
      "1.9210388112848403e-05\n",
      "1.728934930156356e-05\n",
      "1.580157848221378e-05\n",
      "1.4676010972417922e-05\n",
      "1.3547087051462699e-05\n",
      "1.2579437976358219e-05\n",
      "1.1762663799193736e-05\n",
      "4.230908736602146e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 774.04it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataloader = PromptDataLoader(dataset=dataset['validation'], template=my_template, tokenizer=tokenizer,\n",
    "                                         tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "                                         batch_size=4, shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "                                         truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "PromptForClassification(\n  (prompt_model): PromptModel(\n    (plm): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(in_features=768, out_features=768, bias=False)\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(in_features=768, out_features=768, bias=False)\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseReluDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (relu_act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n    (template): ManualTemplate()\n  )\n  (verbalizer): SoftVerbalizer(\n    (head): Linear(in_features=768, out_features=4, bias=False)\n  )\n)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "val_all_preds = list()\n",
    "val_all_labels = list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    val_all_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    # [batch, (1)]\n",
    "    val_all_labels.extend(labels.cpu().tolist())\n",
    "    # [batch, (1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "0.84375"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc = sum([int(i==j) for i,j in zip(val_all_preds, val_all_labels)])/len(val_all_preds)\n",
    "val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 7600it [00:08, 874.64it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=my_template, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "test_all_preds = list()\n",
    "test_all_labels = list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "for step, inputs in enumerate(test_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    test_all_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    # [batch, (1)]\n",
    "    test_all_labels.extend(labels.cpu().tolist())\n",
    "    # [batch, (1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8189473684210526"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = sum([int(i==j) for i,j in zip(test_all_preds, test_all_labels)])/len(test_all_preds)\n",
    "test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}