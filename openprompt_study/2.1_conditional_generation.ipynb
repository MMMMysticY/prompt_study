{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# conditional generation任务\n",
    "条件生成任务是给定一个文本后生成一段推理或者解释\n",
    "prompt的方式是采用Prefix的方式 api:PrefixTuningTemplate\n",
    "文本在<eos>后加上mask\n",
    "在进行推理时，有一些generation_arguments\n",
    "评价方式使用generation_metric api(不一定所有任务都用这个评估)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from openprompt.data_utils.conditional_generation_dataset import WebNLGProcessor\n",
    "from openprompt.plms import load_plm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset_root_dir = '/home/wy/OpenPrompt/datasets/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webNLG_path = os.path.join(dataset_root_dir, 'CondGen/webnlg_2017')\n",
    "os.path.exists(webNLG_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "dataset['train'] = WebNLGProcessor().get_train_examples(webNLG_path)\n",
    "dataset['validation'] = WebNLGProcessor().get_dev_examples(webNLG_path)\n",
    "dataset['test'] = WebNLGProcessor().get_test_examples(webNLG_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025\n",
      "872\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['train']))\n",
    "print(len(dataset['validation']))\n",
    "print(len(dataset['test']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"guid\": \"100\",\n  \"label\": null,\n  \"meta\": {},\n  \"text_a\": \" | Afonso_Pena_International_Airport : elevationAboveTheSeaLevel_(in_feet) : 2988\",\n  \"text_b\": \"\",\n  \"tgt_text\": \"Afonso Pena International Airport has an elevation above the sea level (in feet) of 2988.\"\n}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义prefix template\n",
    "prefix是相对于cloze的另一种prompt的方式 进行文本推理生成任务\n",
    "使用PrefixTuningTemplate api 文本在<eos>后加上mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "my_template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"special\": \"<eos>\"} {\"mask\"}', using_decoder_past_key_values=False)\n",
    "# text加入special eos和mask就是一个基本的prefix template的形式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[[{'text': ' | Aarhus_Airport : cityServed : \"Aarhus, Denmark\"',\n   'loss_ids': 0,\n   'shortenable_ids': 1},\n  {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0},\n  {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}],\n {'guid': '0', 'tgt_text': 'The Aarhus is the airport of Aarhus, Denmark.'}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_example = my_template.wrap_one_example(dataset['train'][0])\n",
    "wrapped_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from openprompt import PromptDataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 18025it [00:16, 1074.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset=dataset['train'], template=my_template, tokenizer=tokenizer,tokenizer_wrapper_class=WrapperClass,\n",
    "                                    max_seq_length=256, decoder_max_length=256, batch_size=5, shuffle=True, teacher_forcing=False, predict_eos_token=True, # 设置predict_eos_token是很重要的 因为template生成的文本最后是<mask>而不是<eos>如果不设置为True会不断进行推理\n",
    "                                    truncate_method='head')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 872it [00:00, 1040.37it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataloader = PromptDataLoader(dataset=dataset['validation'], template=my_template, tokenizer=tokenizer,tokenizer_wrapper_class=WrapperClass,\n",
    "                                    max_seq_length=256, decoder_max_length=256, batch_size=5, shuffle=True, teacher_forcing=False, predict_eos_token=True, # 设置predict_eos_token是很重要的 因为template生成的文本最后是<mask>而不是<eos>如果不设置为True会不断进行推理\n",
    "                                    truncate_method='head')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1862it [00:01, 995.15it/s] \n"
     ]
    }
   ],
   "source": [
    "test_dataloader = PromptDataLoader(dataset=dataset['test'], template=my_template, tokenizer=tokenizer,tokenizer_wrapper_class=WrapperClass,\n",
    "                                    max_seq_length=256, decoder_max_length=256, batch_size=5, shuffle=True, teacher_forcing=False, predict_eos_token=True, # 设置predict_eos_token是很重要的 因为template生成的文本最后是<mask>而不是<eos>如果不设置为True会不断进行推理\n",
    "                                    truncate_method='head')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from openprompt import PromptForGeneration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "prompt_model = PromptForGeneration(plm=plm, template=my_template, freeze_plm=True, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "prompt_model = prompt_model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 模型frozen 仅更新template的参数\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "{\n",
    "    \"params\": [p for n, p in my_template.named_parameters() if (not any(nd in n for nd in no_decay)) and p.requires_grad],\n",
    "    \"weight_decay\": 0.0,\n",
    "},\n",
    "{\n",
    "    \"params\": [p for n, p in my_template.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "    \"weight_decay\": 0.0,\n",
    "},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wy/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-3, eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "total_step = len(train_dataloader) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from openprompt.utils.metrics import generation_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# generation的参数集\n",
    "generation_arguments = {\n",
    "    \"max_length\": 512,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"min_length\": 5,\n",
    "    \"temperature\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"num_beams\": 5,\n",
    "    \"bad_words_ids\": [[628], [198]]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def evaluate(prompt_model, dataloader):\n",
    "    generated_sentence = list()\n",
    "    groundtruth_sentence = list()\n",
    "    input_and_generated_sentence = list()\n",
    "    prompt_model.eval()\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        inputs.cuda()\n",
    "        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)\n",
    "        generated_sentence.extend(output_sentence)\n",
    "        groundtruth_sentence.extend(inputs['tgt_text'])\n",
    "        input_and_generated_sentence.extend((inputs, output_sentence))\n",
    "    score = generation_metric(generated_sentence, groundtruth_sentence, \"sentence_bleu\")\n",
    "    print('test score: ', score, flush=True)\n",
    "    return generated_sentence, input_and_generated_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, global_step 500 average loss: 0.08401759774401261 lr: 0.0009722607489597782\n",
      "Epoch 0, global_step 1000 average loss: 2.2206769032891317e-05 lr: 0.0009445214979195562\n",
      "Epoch 0, global_step 1500 average loss: 0.002482076367018408 lr: 0.0009167822468793344\n",
      "Epoch 0, global_step 2000 average loss: 2.568547551595657e-05 lr: 0.0008890429958391124\n",
      "Epoch 0, global_step 2500 average loss: 1.5708446252915564e-05 lr: 0.0008613037447988904\n",
      "Epoch 0, global_step 3000 average loss: 1.3141183935317713e-05 lr: 0.0008335644937586685\n",
      "Epoch 0, global_step 3500 average loss: 7.719029113104626e-06 lr: 0.0008058252427184466\n",
      "Epoch 1, global_step 4000 average loss: 6.009228314837855e-06 lr: 0.0007780859916782246\n",
      "Epoch 1, global_step 4500 average loss: 4.003010688521158e-06 lr: 0.0007503467406380028\n",
      "Epoch 1, global_step 5000 average loss: 3.7837838099648024e-06 lr: 0.0007226074895977809\n",
      "Epoch 1, global_step 5500 average loss: 3.2953857238737782e-06 lr: 0.000694868238557559\n",
      "Epoch 1, global_step 6000 average loss: 3.7221595317760147e-06 lr: 0.0006671289875173371\n",
      "Epoch 1, global_step 6500 average loss: 2.3041137390009682e-06 lr: 0.0006393897364771152\n",
      "Epoch 1, global_step 7000 average loss: 2.3116575223554037e-06 lr: 0.0006116504854368932\n",
      "Epoch 2, global_step 7500 average loss: 3.602295563140956e-06 lr: 0.0005839112343966712\n",
      "Epoch 2, global_step 8000 average loss: 5.402301142353849e-06 lr: 0.0005561719833564494\n",
      "Epoch 2, global_step 8500 average loss: 1.4480434259382946e-06 lr: 0.0005284327323162274\n",
      "Epoch 2, global_step 9000 average loss: 3.299467779669385e-06 lr: 0.0005006934812760056\n",
      "Epoch 2, global_step 9500 average loss: 8.881616792280056e-07 lr: 0.00047295423023578366\n",
      "Epoch 2, global_step 10000 average loss: 7.427583688723872e-07 lr: 0.00044521497919556175\n",
      "Epoch 2, global_step 10500 average loss: 5.728091477834596e-07 lr: 0.0004174757281553398\n",
      "Epoch 3, global_step 11000 average loss: 4.582837718629662e-07 lr: 0.0003897364771151179\n",
      "Epoch 3, global_step 11500 average loss: 1.396869133287737e-06 lr: 0.00036199722607489597\n",
      "Epoch 3, global_step 12000 average loss: 7.989953789854098e-05 lr: 0.0003342579750346741\n",
      "Epoch 3, global_step 12500 average loss: 1.5303940494817425e-06 lr: 0.00030651872399445215\n",
      "Epoch 3, global_step 13000 average loss: 1.968824955909554e-06 lr: 0.00027877947295423025\n",
      "Epoch 3, global_step 13500 average loss: 8.698796437585088e-07 lr: 0.00025104022191400834\n",
      "Epoch 3, global_step 14000 average loss: 7.331723302144155e-07 lr: 0.00022330097087378643\n",
      "Epoch 4, global_step 14500 average loss: 1.0339900394171764e-06 lr: 0.0001955617198335645\n",
      "Epoch 4, global_step 15000 average loss: 5.166954958752968e-07 lr: 0.00016782246879334259\n",
      "Epoch 4, global_step 15500 average loss: 5.244675054996151e-07 lr: 0.00014008321775312065\n",
      "Epoch 4, global_step 16000 average loss: 1.3286644207681775e-06 lr: 0.00011234396671289876\n",
      "Epoch 4, global_step 16500 average loss: 4.6234002455491916e-07 lr: 8.460471567267685e-05\n",
      "Epoch 4, global_step 17000 average loss: 4.6548324947082167e-07 lr: 5.6865464632454925e-05\n",
      "Epoch 4, global_step 17500 average loss: 5.513880073237943e-07 lr: 2.912621359223301e-05\n",
      "Epoch 4, global_step 18000 average loss: 3.1294718952779023e-07 lr: 1.3869625520110958e-06\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "total_loss = 0\n",
    "log_loss = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    prompt_model.train()\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        global_step = global_step + 1\n",
    "        inputs.cuda()\n",
    "        loss = prompt_model(inputs)\n",
    "        loss.backward()\n",
    "        total_loss = total_loss + loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(my_template.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if global_step%500 == 0:\n",
    "            print(\"Epoch {}, global_step {} average loss: {} lr: {}\".format(epoch, global_step, (total_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)\n",
    "            log_loss = total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:  1.32648207427266e-06\n"
     ]
    }
   ],
   "source": [
    "generate_sentence, input_and_generated_sentence = evaluate(prompt_model, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}